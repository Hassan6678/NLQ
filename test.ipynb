{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fdf91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import Llama\n",
    "import pandas as pd\n",
    "import duckdb, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c014dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your GGUF model\n",
    "MODEL_PATH = \"models/llama-3-sqlcoder-8b.Q6_K.gguf\"\n",
    "\n",
    "# llm = Llama(model_path=MODEL_PATH, n_ctx=2048, n_threads=6)\n",
    "llm = Llama(\n",
    "    model_path=MODEL_PATH,\n",
    "    n_ctx=1024,  # Lower context size if RAM is an issue\n",
    "    n_threads=6,\n",
    "    n_gpu_layers=20,  # Safer for 8GB GPU\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a765f0dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV into DuckDB\n",
    "df = pd.read_csv(\"./data/llm_dataset_v10.gz\")\n",
    "con = duckdb.connect()\n",
    "con.register(\"sales_data\", df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf1163a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt template\n",
    "def build_prompt(nlq):\n",
    "    schema = \"sales_data(region TEXT, quarter TEXT, sales INT)\"\n",
    "    prompt = f\"\"\"### You are an expert Postgres SQL generator.\n",
    "### Given the following table schema:\n",
    "# {schema}\n",
    "\n",
    "### Write a SQL query to answer the question:\n",
    "# {nlq}\n",
    "\n",
    "### SQL:\n",
    "\"\"\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57737dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query model\n",
    "def generate_sql(prompt):\n",
    "    output = llm(prompt, temperature=0, max_tokens=256)\n",
    "    text = output[\"choices\"][0][\"text\"]\n",
    "\n",
    "    if \"SELECT\" not in text.upper():\n",
    "        print(\"âŒ 'SELECT' not found in model output. Raw output:\")\n",
    "        print(text)\n",
    "        return None\n",
    "\n",
    "    # Try to extract SQL statement cleanly\n",
    "    try:\n",
    "        sql = \"SELECT \" + text.upper().split(\"SELECT\", 1)[1].split(\";\")[0].strip() + \";\"\n",
    "        return sql\n",
    "    except Exception as e:\n",
    "        print(\"âŒ Error while parsing SQL:\", e)\n",
    "        print(\"Raw model output:\")\n",
    "        print(text)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe79904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run query\n",
    "def run_nlq(nlq):\n",
    "    prompt = build_prompt(nlq)\n",
    "    sql = generate_sql(prompt)\n",
    "\n",
    "    if not sql:\n",
    "        print(\"\\nâš ï¸ Could not generate valid SQL.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nðŸ“œ Generated SQL:\")\n",
    "    print(sql)\n",
    "\n",
    "    try:\n",
    "        result = con.execute(sql).fetchdf()\n",
    "        print(\"\\nðŸ“Š Query Result:\")\n",
    "        print(result)\n",
    "    except Exception as e:\n",
    "        print(\"\\nâŒ SQL Execution Error:\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0717556",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nlq(\"What were the total sales in Q3 for the Northeast?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de87c5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "con.execute(\"SELECT * FROM sales_data LIMIT 5\").fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b3fc672",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = \"SELECT SUM(sales) AS total_primary_sales FROM sales_data WHERE (month >= 7 AND month <= 9) AND year = 2024\"\n",
    "print(\"\\nðŸ“œ Executing SQL:\")\n",
    "con.execute(sql).fetchdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98fac9ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73323dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, pipeline\n",
    "import os\n",
    "\n",
    "# Set your desired local directory for saving the model\n",
    "LOCAL_DIR = \"./models/t5_small\"\n",
    "\n",
    "# Step 1: Download and save model/tokenizer locally (only needs to be done once)\n",
    "def download_and_save_model():\n",
    "    if not os.path.exists(LOCAL_DIR):\n",
    "        os.makedirs(LOCAL_DIR)\n",
    "        print(\"Downloading T5-small model and tokenizer...\")\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "        tokenizer.save_pretrained(LOCAL_DIR)\n",
    "        model.save_pretrained(LOCAL_DIR)\n",
    "        print(f\"Model saved to {LOCAL_DIR}\")\n",
    "    else:\n",
    "        print(\"Model already downloaded.\")\n",
    "\n",
    "# Step 2: Load model/tokenizer from local dir\n",
    "def load_model_from_local():\n",
    "    print(\"Loading model from local directory...\")\n",
    "    tokenizer = AutoTokenizer.from_pretrained(LOCAL_DIR)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(LOCAL_DIR)\n",
    "    return tokenizer, model\n",
    "\n",
    "# Step 3: Summarize a table (as text)\n",
    "def summarize_table(tokenizer, model, table_text):\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    prompt = \"Summarize the following table:\\n\" + table_text\n",
    "\n",
    "    result = pipe(prompt, max_length=100, do_sample=False)[0]['generated_text']\n",
    "    return result\n",
    "\n",
    "# Example table (as markdown-style text)\n",
    "example_table = \"\"\"\n",
    "| Year | Product | Sales | Region |\n",
    "|------|---------|-------|--------|\n",
    "| 2023 | A       | 1200  | North  |\n",
    "| 2023 | B       | 900   | South  |\n",
    "| 2024 | A       | 1400  | North  |\n",
    "| 2024 | B       | 1100  | South  |\n",
    "\"\"\"\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    download_and_save_model()\n",
    "    tokenizer, model = load_model_from_local()\n",
    "    summary = summarize_table(tokenizer, model, example_table)\n",
    "\n",
    "    print(\"\\nðŸ” Summary of the Table:\")\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9de7c7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'BloomForCausalLM' is not supported for summarization. Supported models are ['BartForConditionalGeneration', 'BigBirdPegasusForConditionalGeneration', 'BlenderbotForConditionalGeneration', 'BlenderbotSmallForConditionalGeneration', 'EncoderDecoderModel', 'FSMTForConditionalGeneration', 'GPTSanJapaneseForConditionalGeneration', 'LEDForConditionalGeneration', 'LongT5ForConditionalGeneration', 'M2M100ForConditionalGeneration', 'MarianMTModel', 'MBartForConditionalGeneration', 'MT5ForConditionalGeneration', 'MvpForConditionalGeneration', 'NllbMoeForConditionalGeneration', 'PegasusForConditionalGeneration', 'PegasusXForConditionalGeneration', 'PLBartForConditionalGeneration', 'ProphetNetForConditionalGeneration', 'SeamlessM4TForTextToText', 'SeamlessM4Tv2ForTextToText', 'SwitchTransformersForConditionalGeneration', 'T5ForConditionalGeneration', 'UMT5ForConditionalGeneration', 'XLMProphetNetForConditionalGeneration'].\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import torch\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./models/bloom_totto\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"./models/bloom_totto\")\n",
    "\n",
    "device = 0 if torch.cuda.is_available() else -1\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e78039e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize_df(df):\n",
    "    # Header row\n",
    "    header = \" | \".join(df.columns) + \" ; \"\n",
    "    \n",
    "    # Data rows\n",
    "    rows = []\n",
    "    for _, row in df.iterrows():\n",
    "        # Convert each value to string and join\n",
    "        row_str = \" | \".join(str(x) for x in row.values)\n",
    "        rows.append(row_str)\n",
    "    \n",
    "    # Combine all rows with ';' separator\n",
    "    table_str = header + \" ; \".join(rows)\n",
    "    return table_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5731de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>Product</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023</td>\n",
       "      <td>A</td>\n",
       "      <td>1200</td>\n",
       "      <td>North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>B</td>\n",
       "      <td>900</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024</td>\n",
       "      <td>A</td>\n",
       "      <td>1400</td>\n",
       "      <td>North</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024</td>\n",
       "      <td>B</td>\n",
       "      <td>1100</td>\n",
       "      <td>South</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Year Product  Sales Region\n",
       "0  2023       A   1200  North\n",
       "1  2023       B    900  South\n",
       "2  2024       A   1400  North\n",
       "3  2024       B   1100  South"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = {\n",
    "    \"Year\": [2023, 2023, 2024, 2024],\n",
    "    \"Product\": [\"A\", \"B\", \"A\", \"B\"],\n",
    "    \"Sales\": [1200, 900, 1400, 1100],\n",
    "    \"Region\": [\"North\", \"South\", \"North\", \"South\"]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2d37ce71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your max_length is set to 150, but your input_length is only 39. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=19)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Summary:\n",
      "Year | Product | Sales | Region ; 2023 | A | 1200 | North ; 2023 | B | 900 | South ; 2024 | A | 1400 | North ; 2024 | B | 1100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 | South ; 2100 |\n"
     ]
    }
   ],
   "source": [
    "table_input = linearize_df(df)\n",
    "\n",
    "# Generate summary/description\n",
    "result = summarizer(table_input, max_length=150, do_sample=False)\n",
    "\n",
    "print(\"Generated Summary:\")\n",
    "print(result[0]['summary_text'])  # or 'generated_text' depending on the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8848e474",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Year | Product | Sales | Region ; 2023 | A | 1200 | North ; 2023 | B | 900 | South ; 2024 | A | 1400 | North ; 2024 | B | 1100 | South'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "table_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db89de4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "| 2025-01-04 |     250 | B         |\n",
      "| 2025-01-05 |     75 | A         |\n",
      "| 2025-01-06 |    125 | B         |\n",
      "| 2025-01-07 |    100 | A         |\n",
      "| 2025-01-08 |     50 | B         |\n",
      "| 2025-01-09 |     30 | A         |\n",
      "\n",
      "**Summary:** From the table, it is evident that Product \"A\" had consistently higher sales compared to Product \"B\". Between January 1st and 9th, Sales for product \"A\" ranged from 75 to 200, while for product \"B\", the sales range was from 30 to 150. Additionally, there is a noticeable pattern of increase in sales for both products on certain days; for instance, sales for Product \"A\" increased from 100 to 200 on January 1st and 2nd, and for Product\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import pandas as pd\n",
    "\n",
    "# Load your model (update the path to your GGUF model)\n",
    "llm = Llama(model_path=\"./models/Mistral-7B-Instruct-v0.1.Q6_K.gguf\", n_ctx=2048)\n",
    "\n",
    "# Sample DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Date': ['2025-01-01', '2025-01-02', '2025-01-03'],\n",
    "    'Sales': [100, 200, 150],\n",
    "    'Product': ['A', 'A', 'B']\n",
    "})\n",
    "\n",
    "# Format as text\n",
    "table_text = df.to_markdown(index=False)\n",
    "\n",
    "# Prompt\n",
    "prompt = f\"Analyze the following table and summarize key patterns:\\n\\n{table_text}\"\n",
    "\n",
    "# Call Mistral\n",
    "response = llm(prompt, max_tokens=256, stop=[\"</s>\"])\n",
    "print(response[\"choices\"][0][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f519a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# from ollama import Ollama\n",
    "\n",
    "# # Step 1: Create a sample pandas DataFrame\n",
    "# df = pd.DataFrame({\n",
    "#     'Product': ['A', 'B', 'C'],\n",
    "#     'Sales': [100, 150, 90],\n",
    "#     'Month': ['June', 'June', 'June']\n",
    "# })\n",
    "\n",
    "# # Step 2: Convert DataFrame to markdown table\n",
    "# table_text = df.to_markdown(index=False)\n",
    "\n",
    "# # Step 3: Create a prompt for summarization\n",
    "# prompt = f\"\"\"\n",
    "# You are a helpful data analyst.\n",
    "\n",
    "# Given the following table, summarize the key insights, trends, or anomalies:\n",
    "\n",
    "# {table_text}\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 4: Use Ollama Python client to send prompt to Mistral\n",
    "# client = Ollama()\n",
    "# response = client.chat(model='mistral', messages=[\n",
    "#     {\"role\": \"user\", \"content\": prompt}\n",
    "# ])\n",
    "\n",
    "# # Step 5: Print the response\n",
    "# print(\"\\nðŸ§  Summary:\")\n",
    "# print(response['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
